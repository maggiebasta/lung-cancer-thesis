{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from skimage import io\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    img_to_array,\n",
    "    load_img\n",
    ")\n",
    "\n",
    "from data_generator_3d import *\n",
    "from cnn_baseline import *\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = ImageDataGenerator(\n",
    "#     rotation_range=90,\n",
    "    rescale=1./255,\n",
    "# #     shear_range=.1,\n",
    "#     zoom_range=.15,\n",
    "#     brightness_range=[.85, 1.0],\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "label_generator = ImageDataGenerator(\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "# train_generator = image_generator.flow_from_directory(\n",
    "#     directory=\"data/nlst_train/image_full\",\n",
    "#     color_mode=\"grayscale\",\n",
    "#     target_size=(256, 256),\n",
    "#     batch_size=12,\n",
    "#     class_mode=\"binary\",\n",
    "#     shuffle=True,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# train_generator = image_generator.flow_from_directory(\n",
    "#     directory=\"data/nlst_train/image_roi_2d\",\n",
    "#     color_mode=\"grayscale\",\n",
    "#     target_size=(32, 32),\n",
    "#     batch_size=16,\n",
    "#     class_mode=\"binary\",\n",
    "#     shuffle=True,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "train_generator = generator(\n",
    "    base_directory='data/nlst_train/image_roi_3d',\n",
    "    input_gen=image_generator,\n",
    "    target_gen=label_generator,\n",
    "    batch_sz=2\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAADjCAYAAADt28tlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3dmSHMX9NuAC/hiJkUajZRCLbBZBgB1eCMIHOLDDl+Sr8m34EhwE5sQYEEhIQrtG0khshu+gPk9nvlOqRAHZ3TDPczSpqq7KyurfQare6nzsu+++GwAAAKCXx1fdAQAAAH7eTDwBAADoysQTAACArkw8AQAA6MrEEwAAgK5MPAEAAOjKxBMAAICuTDwBAADo6v+WebK//e1v35Xtb7/9tu7M/9Xd+e67avfhySeffOi23d3dqr21tVW1v/jii4ceaxiG4b///e9s+xe/+MVsX0+cOLH39927d6tt2dcvv/yyat+5c6dqP/HEE7N9+frrr6v2xsbG7PEPHTq09/dXX31Vbcvryu15j1J+Pq+1PPcwDMNjjz1Wte/du/fQffM6H3+8/n+SHKfcP+953rMHDx5U7W+++aZql9ee585xye15ruzb3//+93og1oQaHanRBTW6XtToSI0uqNH1okZHanRBjRbHyH8AAACAH5OJJwAAAF0tNWqb8tF1PqLNR9Xl9qeeeqralu2MI+Tj4Pv371ftw4cPz/YlH41nfKF87J5xgXykn33Jvuej7bk4wdT55h59Z1/ysfmVK1eqdsY4sm/ZzihEjmPuX34HMrqQ8vuws7NTtfO6c1wybpDny+OX8h61YhrZznv2U6FGp/uuRqep0eVTo9N9V6PT1OjyqdHpvqvRaT/3GvXEEwAAgK5MPAEAAOjKxBMAAICulvqOZ+tnnudyx8NQ55ozw5yZ5yNHjlTtzc3Nqr29vV218+eFy58+nurbRx99VLXLfHfm1I8dOzZ7rtz/6NGjVTuvNfPcrZ/bLo+fY57Hznx33rM8d/6cdubc83yZF3/66ae/97la70XkOGamP8+d7zpkVr08/s2bN2ePnX3Jvuc9X1dqdPpcavT7nUuN9qdGp8+lRr/fudRof2p0+lxq9Pud6+deo554AgAA0JWJJwAAAF2ZeAIAANDVUt/xzHVyMuec2eHMCpcZ7dY6NpnHzmPdvn27amdmOttlPnsY9mfNy/apU6eqbZmnzjV58rpv3bpVtTNjnWtCtdZxKrPnmR3P9wNyzafse3riiSeqdmvc8/zl5/Me5j3Kz2YWPc81t/7UMOwft3w3ouxP9i0/m33Le9B6p2NdqNGRGp3+vBpdPTU6UqPTn1ejq6dGR2p0+vMHvUY98QQAAKArE08AAAC6MvEEAACgq5Wu45lZ4ZRZ4TLvnevUpI2NjaqdOfbMa+e5Dh06NNvXzJ7Pnau1zk1mzzNTndeaufnMa+d6ReXxst9Xrlyp2pk1zzWesm+Zk89seWtcy+9EjnEee+77MAzttY1y3DObntde3rc8d15nfp9ye34H1pUaHanRBTW6XtToSI0uqNH1okZHanRBjRb9be4BAAAAP4CJJwAAAF0tNWqbj83zsXs+Ps5HuOWj8PzJ53xEn5/NY+e5W3GGPF566qmn9v7Ox+gZu7h+/XrVfu+996p2/nRyxgnyWl599dWqnZGBixcv7v3dilVkOx/Z52P0vA/5mD77nj+fXf4MdH726NGjs589fPjw7LkzftCKeWRkpYw/5LjluOT3I6MNGU9YV2p0pEYX1Oh6UaMjNbqgRteLGh2p0QU1WvS3uQcAAAD8ACaeAAAAdGXiCQAAQFdLfcezzIYPw/7scLYz11xmiTOnnvnrzFvnzxFnfjtzynn83H7jxo2qXWb6y5z5MAzD1atXq3bmrTNr3vr54szRnz9//nsfP68rxzjl9hy37Ev2NT//7bffVu3yPuVnW/eg9VPdmVW/du1a1c6cfB6/zM23fjq79XPWrXFeF2p0pEYX1Oh6UaMjNbqgRteLGh2p0QU1WhyjuQcAAAD8ACaeAAAAdGXiCQAAQFdLfcczs+eZHc5scWaJ79y5s/d3ue7MMOxfLyi3Z4Y61//J7bkOU2v/Mlue2e7Mc+d1Zj47c+s5Tvn+QPYlM9fluOe6SHnsvEd3796t2vl+QWvNnzS3flHesxy3bLdy77dv3579fN6nbJfXluOW597c3Jzdnu9hrCs1OlKjC2p0vajRkRpdUKPrRY2O1OiCGl3wxBMAAICuTDwBAADoysQTAACArpb6juc//vGPqp1r+mS2+MyZM1X7448/3vs789hPPvlk1W7ljPPcmefOnHseP7PnZZ47M8+tfHWue5PZ88zJt3Lz2ffy/YFWZv5RM/Y5Ltm3lPetzItnLj3XUcpzpVxnKfvSWt9qY2OjapfvUuS6R3nsvKf5ffqpUKPTbTU6UqOrp0an22p0pEZXT41Ot9Xo6KDXqCeeAAAAdGXiCQAAQFcmngAAAHT1WCun/GM6dOhQdbJWZjqzw2VOuVzn6P8fu2q38tmZec7ccm6fy7lPteeOlTKjn/nsPHdrzZ88Xo7F3LEzs9/Kwee5si857nn8cnvm9TNrnn3J7fkuQ+vzLeV9y3W2Wvckz53t8+fPP1pnlkSNTlOjIzW6emp0mhodqdHVU6PT1OjooNeoJ54AAAB0ZeIJAABAVyaeAAAAdLXUdTwzl5xr+rTy2uXnMxs+l6cehv3r3uTnH3WNnsxBz22b23cY9mfuW3ntPF6OW36+XDOqtY5Sa42eXJso5fF3dnaq9lxmP/uS++baV613D3Kc8vuUfc33E8pxPX36dLUtxyHHvDWu60qNTlOj031Ro8unRqep0em+qNHlU6PT1Oh0Xw5ajXriCQAAQFcmngAAAHRl4gkAAEBXS33HM7PBmdfOXHOuV1Rm0TOX3sqOZwY6P99au+b+/ftVO3Pyc+d+1DV3WusBZX47ryX3L/PjrXWRHuVYU/u3xjG37+7u7v19/Pjxalu+q5AZ/Dx3jmt+Pse1NY7l9y/X0srryO/flStXZs+9rtTo9HY1OlKjq6dGp7er0ZEaXT01Or1djY4Oeo164gkAAEBXJp4AAAB0ZeIJAABAV0t9xzNzxZmhzlzznNa6NZnXzvx1K0uemftWbrlcyybXtcljt/L/+fl8HyAz1tnXPN/c+kE3btyo2qdOnarat27dmj1XjksrH57vD2xubu79nWsL5fchP5vy/YK85631rfL7Wb6PkNeZaxvl9tZ3YF2p0ZEaXVCj60WNjtToghpdL2p0pEYX1OiCJ54AAAB0ZeIJAABAVyaeAAAAdPVYZn17OnToUHWyzAZnFv3w4cNVu8wxZ14789WZqW/ljjNzvbGxUbVz/aHMWJefzwx0fraVU89rybx2S45jmbPPY2dfM1Offc9jZyY/71mOU96XPP5c33LffF8g72Hun9feWlOqPF5rzae8zlynKbffuXPn4Re+Qmp0pEYffvy5vqnR/tToSI0+/PhzfVOj/anRkRp9+PHn+vZzr1FPPAEAAOjKxBMAAICuTDwBAADoaqnreGbOfXd3d3b7vXv3HnqszDjnekG5Lk7muTO3nBnozHO31sEpzze3fs8w7M+5Z3772Wefne1r5uDz2nNsyr7mOGRfMreecly3traqdq6FlOOWyvPn+wA7Ozuzfctj57Vlzj3HJe9xZvLLcc9jZ19ye2sdpnWlRkdqdPr8anT11OhIjU6fX42unhodqdHp8x/0GvXEEwAAgK5MPAEAAOhqqVHbfGSb8YPWzziXj3jzMXg+or97927VPnLkSNXOx8OtZWVy//LnroehfnSdfcvH3vlY/NSpU1U7H+lfvny5amfcIK89fyq53D+vc+6R+1TfMyJw7dq1qp1Rizx+Ko+fUYijR49W7YwL5Pel9TPhGRHIezh3rXndd+7cmT12jmMrhrEu1OhIjU4fX42unhodqdHp46vR1VOjIzU6ffyDXqOeeAIAANCViScAAABdmXgCAADQ1VLf8cyscGai86eWM9dc5rkzV5wZ+cyGt35OOLPo2ZfMSOf+5c8+57bM4Gf+P/fP/Hfrp5Qzk53Ka8/ryHHLXHyeK9uplUWfy6a3fiK6dc+z7/n5fM8i34XIz5fvH+TPeue58x7lOLTGbV2o0ZEaXVCj60WNjtToghpdL2p0pEYX1GjR3+YeAAAA8AOYeAIAANCViScAAABdPdZa0+fHtL29XZ0ss+mZLc6cc5lVz2352cx3t3LImR3P/fN8c+OWGftco+fevXtVO8ehNS65dlH2LXP1ZY4+1/vZ2dmp2plLz77kuTNb3up7jnuuAzXXt8yW5/sB2Zc89ty7ClN9K9t5rtaxc5zy8/fu3au/YGtCjY7U6IIaXS9qdKRGF9ToelGjIzW6oEaLY+Q/AAAAwI/JxBMAAICuTDwBAADoaqnreGbeO/PZmVXPbHG5/ezZs9W2Tz/9tGrfv3+/auf6QpnPzpx7K4ueWfNy3abMlrfeo811lrLvrTV8ctzy82Xfc1vmtVOu/5PXnXnu1riW6wXl5/N9gfxsa42n/D7luwyt719eSznup06dqra1MvP5/kCO+7pSo9PU6EiNrp4anaZGR2p09dToNDU6Oug16oknAAAAXZl4AgAA0JWJJwAAAF0tdR3Pp556qjpZZqg3NzerdmaFy/ViMgv+4osvVu3MKf/73/+ePXaOQ+a3Mx9+/Pjxqr27u7v3d15Xa72g8rNT21v57Dx+KnP1eR1lXn8Y9ufSM8N/8eLF2XO99tprVfvYsWNV+8KFC1X7ypUre39nrr31LkLm2vNaUo7T9vZ21c5cfHnteY/yHrTqKLd/8803a7n+mBqdPrYaHanR1VOj08dWoyM1unpqdPrYanR00GvUE08AAAC6MvEEAACgKxNPAAAAulrqOp65/kuuXdTKopfZ48w8nzt3rmrfvHmzaucaPWWGfhiG4cGDB1U7c/XZvnHjxkP7mmv05HVm9jzHJdcqynWZci2kbGfuvsyLZ/Y7c+7PPPNM1b506dLsuTKL/vzzz1ft9957r2pnNr3MtrfWMkqZU89xy+PluOf3K+9DOe65La87vx95D5599tnhp0CNjtToghpdL2p0pEYX1Oh6UaMjNbqgRov+NfcAAACAH8DEEwAAgK5MPAEAAOhqqe94Zu44s8HZTmV+vJX93tnZqdqZHT969GjVzjx4az2hzM2XmevMvT9qPjv3z4x/5rVbefEy83/ixIlq28mTJ6v2J598MnuszPCfPXu2apdrFQ3DMNy6datq57iV+fHMlrey5q1xSXkteU/zO1QeP9/JaOXc8/v1zjvvzPZtXajR/fsOgxr9HzW6emp0/77DoEb/R42unhrdv+8wqNH/Oeg16oknAAAAXZl4AgAA0JWJJwAAAF0t9R3PzB1nXjvz4JlLLjPXmYnOrHnrXLmWUeaW8/iZ157Ltue5MlOfmelWHjvz3NnX3H9jY6Nql9n0t956q9p2+fLlqn3hwoXZY7344otVO6/1s88+q9qnTp2q2jkWZbY8t+U9yOvMHPwXX3xRtfOe5XpU+W5D3odyLa2Ufc2c+5///Oeqvb29/dBjrRM1OlKjC2p0vajRkRpdUKPrRY2O1OiCGl3wxBMAAICuTDwBAADoysQTAACArpb6jmfmtTO3nGsf5VozZQ66lXPPc7XWPirX/xmG/WshZc45M9Xl8Vtr8LTWycljtzLWmQd/7rnnqnaZXb906VK1LdceyrWONjc3q/YzzzxTtd99992qPXfPhmF/1rzcvzXmrXuQ34Ec1+PHj1ft/A7k/mXfc02n/P69/fbbVfuFF16o2rle1bpSoyM1Or2/Gl09NTpSo9P7q9HVU6MjNTq9/0GvUU88AQAA6MrEEwAAgK5MPAEAAOhqqe94ZiY6c8bZzhx8mXvODHTmijMTPbdO0tT+W1tbVTtzzrkOU9mfXGMn1yZqZcFz/xy3vPaU++e1lHJtojt37lTtXAvpX//6V9U+d+5c1c5ry3auKVW+I5BjnO8L5D3MdxVS612HvOc57mU7+/3OO+9U7VdeeaVqX716tWrn+wPrSo2O1OiCGl0vanSkRhfU6HpRoyM1uqBGFzzxBAAAoCsTTwAAALpaatQ2Hy/nY/d8vJzxg3L7qVOnqm3588T5M8957nyc3PoZ53yMnn0tt+cj/Xz0nJ/Nn8NOeS0Z08ixuHnzZtX+5JNP9v4+ceJEte3MmTNV++WXX67a+Qg/+54/65zjlH3PcS/3z7hBfjbjAhk5yb5knCDjCvmdyNhIef7f//731bazZ89W7db3L39WfF2p0enPqtGRGl09NTr9WTU6UqOrp0anP6tGRwe9Rj3xBAAAoCsTTwAAALoy8QQAAKCrlS6nkjnj/NnmzFiXMvO8sbExuz0zzpmpz8x1Zqgzt5xZ9TI3nz9fffv27aqd15kyM52yb/mz0MePH6/av/nNb/b+bmXkMxue43T69Omqnffwxo0bVTtz8Hnt5Ti2fjY8c+z509l5jzInv7u7W7Vb41z+jHTm3nPMs2957Na7DetCjY7U6IIaXS9qdKRGF9ToelGjIzW6oEYXPPEEAACgKxNPAAAAujLxBAAAoKuVvuOZWeHMBmcGu8waZx4725lxTrnGTyvnnusJpXI9ozx3rnU0t4bOMOzP4Od6QNm3HMfMlpf57+xLZsdzHLOv165dm90/x/HWrVsP7UvKvuX9z3HNccnvT45rrjGVfXn77berdrl+Ub4fkMfOdbpSZvjXlRodqdFpanT11OhIjU5To6unRkdqdNpBr1FPPAEAAOjKxBMAAICuTDwBAADoaqnveP7qV7+q2ufOnavarex6uWZQbjt58mTVPnPmTNXO3Hquo5PrKOXaNbleUa7ZU2aoM3+dufQ8d8qseJ47M9d5vLzWDz744KH75rsImT3Pa7lw4ULVznHKrHrK/cu8eObYM0Offc9xzb7nelbZt7/+9a9VO3Pw5fnyfmdfsy/57kLew3WlRqfPndToSI0unxqdPndSoyM1unxqdPrcSY2ODlqNeuIJAABAVyaeAAAAdGXiCQAAQFdLfcfz6NGjs+3MRGfuucxgZ444M82Z175+/XrVzhxzruGTefBcTyhzzeXxMneeeezM92dmuiX3b/X1n//8597fOeaZ989s+fHjx6v2888/P3uunZ2dqp33MK+9vE85TjmOmffPNZzu3r07u3/es8y5Z9/KsclxyevKcc1xyf3XlRodqdEFNbpe1OhIjS6o0fWiRkdqdEGNFudv7gEAAAA/gIknAAAAXZl4AgAA0NVS3/HMtWZeeumlqv3uu+9W7cxkb21t7f39xhtvzB779u3bVfvevXuz7Y2NjardWqMnc/VlRjsz9ZlLz89m3jv3z2vLPPhzzz1XtfPayvPlmjyZz87rvnLlStXOLHl5T4ZhGG7evFm1M4termWUx8vceY5LtlvrCeU4te5pjns5VnmdOW7Zzr7ld2JdqdHpz6rRkRpdPTU6/Vk1OlKjq6dGpz+rRkcHvUY98QQAAKArE08AAAC6MvEEAACgq6W+43n16tWqffbs2aqdWfbPPvusar/22mt7f+daMh999FHVznWScu2iXBspt2fmPtfRyex6mXPOzHO2s++7u7tVO/PbmR1v9T3z4dvb23t/37hxo9qW2fDsa+bcr127VrWPHTs2u39uz3Es1/zJdZFyjLOd6wW1cu5vvfVW1c53AObWQsrvQ15X3oNcCyn7tq7U6HTf1ehIja6eGp3uuxodqdHVU6PTfVejo4Neo554AgAA0JWJJwAAAF2ZeAIAANDVUt/xzPVfPvnkk6p95syZqv3KK69U7c3Nzb2///Of/1TbMlOfmejMmqdWzj3XF8p8eLk2Umacn3zyyaqdOfbcnnnszLnn+kCZsc68d3m8XPco9838dq751Op7trNvqczR5xhnDj3lsTPv/7vf/W62b3nP53LwrQx+a12mnwo1OlKjC2p0vajRkRpdUKPrRY2O1OiCGi2O8cifAAAAgEdg4gkAAEBXJp4AAAB0tdR3PDMPnusPZVY92+fOndv7+/z584907ieeeKJqZ/67td5Q5pjncvG5b15n5tbzXCdPnqzaOW6ZD0+3bt2q2mV/st/ffvtt1c789vXr16v21tZW1c7seebkL1++XLUze17el9aaTdn33J59y7WMcl2mPF6u+1SuX9TqS15Xjmu215UaHanRBTW6XtToSI0uqNH1okZHanRBjS544gkAAEBXJp4AAAB0ZeIJAABAV0t9xzOzv5kdzvWCMt/96aef7v2d2fLMtee6OK21iVL2LdeymVuHJz+b2fDsa+bgM2Od52odP6+9zM3nZzOTf+jQodm+tvLcOzs7VTuz7Ldv3x4eJvfNvmb+P/v6pz/9qWqXufVh2P8+QK5llNea5yvlGOf3I9/ZyO3rSo1O91WNTu+rRpdPjU73VY1O76tGl0+NTvdVjU7ve9Bq1BNPAAAAujLxBAAAoKulRm03Nzfrk8cj3Hw8/NVXXz30WPk4Nx9FZzuPnX3J7flTya3HyXOPqrMvGZ3I+EAeq7V/K0pRjmPum9eV0Yfcnj+lnH3Nn8vOR/pzP818//792XPldb/88stV+8SJE7N9ee6556p2GWeZOt9cZCB/ijtjGHnd+X1aV2p0pEanj6dGV0+NjtTo9PHU6Oqp0ZEanT7eQa9RTzwBAADoysQTAACArkw8AQAA6Gqp73gePXq0ar/66qtV+913363amTsuM9OZBc/ccf50cmaa86eQM4Of+fDMXGeWvcxY57HSXJ5/GNpZ88yHtzL4ZUa7lXOfy6VP7b+1tVW1X3rppap9/vz5qp3jWN6HR/1p7j/+8Y+P1NfMnmffP//886pdjmN+tpXRz75me12p0ZEaXVCj60WNjtToghpdL2p0pEYX1OiCJ54AAAB0ZeIJAABAVyaeAAAAdLXUdzzv3r1btd9///2q/eWXX1bte/fuVe2zZ8/u/Z259lzHJjPT2c78d8uDBw+qdubuDx8+/NC+ZL46M9Mp89p57qeffrpqZ+Y6190pj5fbsm957rzOVuY++5r3Kft+/Pjxvb9zfansa65llO9RXL16tWrnWkaXL1+u2seOHavamdm/dOnS3t/53c3r3t7ertr5Xf6prD+mRkdqdEGNrhc1OlKjC2p0vajRkRpdUKMLnngCAADQlYknAAAAXZl4AgAA0NVS3/HMrHDm2jMbnBnpcu2ZzCFvbm5W7dZaRbm+UK49kzn5Mp89DMNw7dq1ql2uy5Pnzgx0niuz5rluUo5btlOu81SeL/vyqMcu8/3DsL/vrUx/5uLLvrXWB/rLX/5StfNasi/5/cnvQH4+93/99df3/s7vW2b0s685jo/6nsWqqNHpc6nRkRpdPTU6fS41OlKjq6dGp8+lRkcHvUY98QQAAKArE08AAAC6MvEEAACgq5Wu45nZ8taaPTdv3tz7O3PomWHOjHNmyXP/zIrnmj47OztV+8SJE1W7vJZckyevMzPRmZnOvrSOl30tM/jDUF/r3LpHU9tzHL/++uuqnXnuVsY/j1eORW574YUXqnZrHLMv+W5DvleR43z79u2qXb5nkeOS6yLlPcj9c42ndaVGR2p0QY2uFzU6UqMLanS9qNGRGl1Qo8VnmnsAAADAD2DiCQAAQFcmngAAAHS11Hc8M5ec69xkDnl3d7dql1nizGtnZjrbuY5SZuxba/pkhjoz/GWmunWdmcHPPHf2vZXJz8/n+kLlWkut60z57kGOQ/attfZR9r281szrv/nmm7N9yevc2tqq2plz//DDD6t2vjuR96XM+Od15tpGR44ceehnp861rtToSI0uqNH1okZHanRBja4XNTpSowtqdMETTwAAALoy8QQAAKArE08AAAC6Wuo7npmBzpxxZtMzI13mtzPTnGvHZEY6c8uZS87seHmuYdif5878eJltz0x8rnOT+e5c/yevO4/XWssoc/Nlxr+VW892HivHdXNzs2pnxr+V6S+v5eTJk9W2VpY829vb21X74sWLVTvfdci1jPJajh49OjxMZu5zTae8J3kP15UaHanRBTW6XtToSI0uqNH1okZHanRBjS544gkAAEBXJp4AAAB0ZeIJAABAV0t9xzPz3Zlbzqzw3Do4mZHPY2f+OjPPrb6kzGvn/mVGOvuWax3lmk153fk+QGbwW33LvHjZn8y1Zy49xyXPndeS7xvkuOT+efzyXYiXX3652pYZ+2vXrlXtXMsoz5XrD2U2PY+f96W8ltbaWadPn67arXW71pUaHanRBTW6XtToSI0uqNH1okZHanRBjS544gkAAEBXJp4AAAB0ZeIJAABAV0t9x/PNN9+s2levXq3amVPO9WDKPHiuF5S54hMnTlTtmzdvVu3W+kKZB88c89z+mafO9YRa5851czLPndfeynuX7w9k/r+Vz87ce2b6M6Ofaz611koqx+LFF1+stuX9z7WGci2knZ2dqp3jmvclc/DZLt8fyOvKY2f+P989yHW91pUa/X7nVqMjNbp8avT7nVuNjtTo8qnR73duNTo6aDXqiScAAABdmXgCAADQlYknAAAAXS31Hc+LFy9W7cwhv/HGG1U7M9UffPDB3t9XrlyZPVZmwTMznWvwZK45ZV8y91xmpjPr3Tp2ruGUa/Zkbj6z6Jldz2x5uX9ed+azW1nwVpY8z50Z/sz8v/rqq3t/57hl3zY3N6t2vpuQ7wPkuOT+OY55j8u+57aU37c8do7bulKj09ToSI2unhqdpkZHanT11Og0NTo66DXqiScAAABdmXgCAADQ1VKjtpcuXara+Uj2woULVTvjCMeOHdv7O38qufXz1BsbG1U7IwH52Dz3z+PlY/TycXMeay4OMHWu1k9Kt+Sj8vJ8GRfIn4xuHSujEPn5VtQi7/nZs2f3/s6ISOunt3Nc8/M5rhk/yLHI71AZA8m+tL4/2c5oxbpSo/v3nTqXGp3eV432p0b37zt1LjU6va8a7U+N7t936lxqdHrfn3uNeuIJAABAVyaeAAAAdGXiCQAAQFdLfcczc8mZQ84s8aefflq1y59Hzp8bzhx8Hjuz5Pkzzpl5vn379uz+efwyF99yH8u7AAADeElEQVTKSG9vb1ftX//617Pnzr5lO7PpmbG+c+fO3t95HXM/qzwM+3PtJ0+efOixp473xRdfVO08f3m8fB8gfw47f+76+PHjVfvGjRtV+5e//GXV/vDDD6v2iRMnqnZ+R8p2jkP+/HX+THh+P34qPwOvRkdqdPp4anT11OhIjU4fT42unhodqdHp4x30GvXEEwAAgK5MPAEAAOjKxBMAAICulvqOZ+a/M5+dmenMEpdr0WT2+/Dhw1U7c8eZk3/22Wdn+5KZ68yi5+fLa8tj5XW88sorVTtz8NeuXavaly9frtqZoc7sea67VOa35/L6w7D/HuS7Crm2Ua7hk/nwPN7rr7/+0L5ev3692pa59HJtq2HYn9FvZfJz+4MHD6p2vjtR5uxzW97T/L7kPcjM/rpSoyM1Ot1XNbp6anSkRqf7qkZXT42O1Oh0Xw96jXriCQAAQFcmngAAAHRl4gkAAEBXS33H8/HH63luZqgzG5w55zKjnevQZO64lQ0/depU1f7tb39btS9cuFC1z507V7WvXLlStTNPXsr8dmbw33///ar90UcfVe3M0ec4Zea/fD9gGOqxyXHKe5KfzSz66dOnq3Yr/51rG+W4l9n1vN957My5Z6Y+303Y2Nio2pnRz2x6jkV5LbnuUn53c3vrXOtKjY7U6IIaXS9qdKRGF9ToelGjIzW6oEaL8zf3AAAAgB/AxBMAAICuTDwBAADoaqnveGZ+OzPWrWxxZtnnPptyXaUbN27MnusPf/hD1c7s+scff1y1P//8872/M6995MiRqv3ee+9V7YsXL1btzEhn3zPnnnZ2dh66LT+b2fDMluc9ymz5XN5/GIbhzJkzVTuz6mV/8ty5rlKOQyu3npn71rsS+fny+DluuS5S5tx3d3erdl7LulKjIzU63R81unpqdKRGp/ujRldPjY7U6HR/DnqNeuIJAABAVyaeAAAAdGXiCQAAQFePZV4XAAAAfkyeeAIAANCViScAAABdmXgCAADQlYknAAAAXZl4AgAA0JWJJwAAAF2ZeAIAANCViScAAABdmXgCAADQlYknAAAAXZl4AgAA0JWJJwAAAF2ZeAIAANCViScAAABdmXgCAADQlYknAAAAXZl4AgAA0JWJJwAAAF2ZeAIAANCViScAAABdmXgCAADQlYknAAAAXZl4AgAA0NX/A2AU0kKaCYv+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_augmentation('data/nlst_train/image_roi_3d/25/5.tif', image_generator, n_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_logits(y_pred):\n",
    "    y_pred = tf.clip_by_value(\n",
    "        y_pred, tf.keras.backend.epsilon(),\n",
    "        1 - tf.keras.backend.epsilon()\n",
    "    )\n",
    "\n",
    "    return tf.math.log(y_pred / (1 - y_pred))\n",
    "\n",
    "\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    y_pred = convert_to_logits(y_pred)\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        logits=y_pred,\n",
    "        labels=y_true,\n",
    "        pos_weight=100\n",
    "    )\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# model = cnn_baseline()\n",
    "# model = cnn_baseline(input_shape=(32, 32, 1))\n",
    "model = cnn_baseline_3d(input_shape=(50, 50, 50, 1))\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(lr = 1e-5),\n",
    "#     loss=tf.keras.losses.binary_crossentropy,\n",
    "    loss=weighted_cross_entropy,\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.SpecificityAtSensitivity(.5),\n",
    "    ]\n",
    ")\n",
    "# model_checkpoint = ModelCheckpoint(MODEL_NAME, monitor='loss',verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Epoch 1/15\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "Found 1182 images belonging to 1 classes.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 0.6545 - acc: 0.4150 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5133 - specificity_at_sensitivity: 0.5181\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - 27s 55ms/step - loss: 0.6484 - acc: 0.4170 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5013 - specificity_at_sensitivity: 0.4868\n",
      "Epoch 3/15\n",
      "  4/500 [..............................] - ETA: 26s - loss: 0.6593 - acc: 0.3750 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.6000 - specificity_at_sensitivity: 0.6667        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aab57428244a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=500,\n",
    "    epochs = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir()\n",
    "with open(f'{pid_roi_3d_path}/{im}.pkl', 'rb') as input_file:\n",
    "        cube = pkl.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator, steps=181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    img_to_array,\n",
    "    load_img\n",
    ")\n",
    "from skimage.io import imread, imsave\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img('data/nlst_train/image_roi_3d/label/545.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imread('data/nlst_train/image_roi_3d/label/545.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsave('test.tif', np.array(Image.fromarray(np.array([[0]]), 'L')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img('test.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path='data/nlst_table_cleaned.csv'\n",
    "df_nlst = pd.read_csv(table_path)\n",
    "df_recurr = df_nlst[['pid', 'recurrence']].drop_duplicates()\n",
    "\n",
    "RecurTable = {pid: recurr for _, (pid, recurr) in df_recurr.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 112500 into shape (50,50,50,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2d378a65c24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mcube\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcube\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mYs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecurTable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 112500 into shape (50,50,50,1)"
     ]
    }
   ],
   "source": [
    "Xs = []\n",
    "Ys = []\n",
    "for pid in os.listdir('data/nlst_rois_3d'):\n",
    "    pid_roi_3d_path = f'data/nlst_rois_3d/{pid}'\n",
    "    for im in os.listdir(pid_roi_3d_path):\n",
    "        with open(f'{pid_roi_3d_path}/{im}', 'rb') as input_file:\n",
    "            cube = pkl.load(input_file)[:50][:50][:50]\n",
    "            \n",
    "            Xs.append(np.array(cube).reshape(50, 50, 50, 1))\n",
    "            Ys.append(RecurTable[int(pid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array(Xs)\n",
    "Ys = np.array(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 780 into shape (780,50,50,50,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-54641ba38d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m780\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 780 into shape (780,50,50,50,1)"
     ]
    }
   ],
   "source": [
    "Xs.reshape(780, 50, 50, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
